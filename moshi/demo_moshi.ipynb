{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Moshi: a speech-text foundation model for real time dialogue\n",
        "\n",
        " [Moshi][moshi] is a speech-text foundation model and **full-duplex** spoken dialogue framework.\n",
        " It uses [Mimi][moshi], a state-of-the-art streaming neural audio codec. Mimi processes 24 kHz audio, down to a 12.5 Hz representation\n",
        " with a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size),\n",
        " yet performs better than existing, non-streaming, codecs like\n",
        " [SpeechTokenizer](https://github.com/ZhangXInFD/SpeechTokenizer) (50 Hz, 4kbps), or [SemantiCodec](https://github.com/haoheliu/SemantiCodec-inference) (50 Hz, 1.3kbps).\n",
        "\n",
        " Moshi models **two streams of audio**: one corresponds to Moshi, and the other one to the user.\n",
        " At inference, the stream from the user is taken from the audio input,\n",
        "and the one for Moshi is sampled from the model's output. Along these two audio streams, Moshi predicts text tokens corresponding to its own speech, its **inner monologue**,\n",
        "which greatly improves the quality of its generation. A small Depth Transformer models inter codebook dependencies for a given time step,\n",
        "while a large, 7B parameter Temporal Transformer models the temporal dependencies. Moshi achieves a theoretical latency\n",
        "of 160ms (80ms for the frame size of Mimi + 80ms of acoustic delay), with a practical overall latency as low as 200ms on an L4 GPU.\n",
        "\n",
        "\n",
        "For more information, checkout our repo\n",
        "[[repo]](https://github.com/kyutai-labs/hibiki),\n",
        "[[samples]](https://huggingface.co/spaces/kyutai/hibiki-samples), and [[paper]](https://arxiv.org/abs/2410.00037)."
      ],
      "metadata": {
        "id": "iuzciRNiOznZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYz-oOcaOwwr"
      },
      "outputs": [],
      "source": [
        "!pip install \"git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&subdirectory=moshi\"\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also run the model through our WebUI for live translation.\n",
        "# Click on the gradio.live link!\n",
        "! python -m moshi.server --gradio-tunnel --hf-repo kyutai/moshiko-pytorch-q8 --half\n",
        "# Or with Moshika's voice\n",
        "# ! python -m moshi.server --gradio-tunnel --hf-repo kyutai/moshika-1b-pytorch-bf16 --half | grep -v 'frame handled'"
      ],
      "metadata": {
        "id": "jykmkKO0f7dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouGqgLmWL0x0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}